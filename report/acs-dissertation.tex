% TODO JAMES where to write more? which sections are weakest?
% TODO JAMES We will demonstrate these ideas by building a system that can choose between different machine learning methods, doing so efficiently by running approximate versions of the algorithms to estimate which methods are most promising to run with less approximation
% TODO JAMES O runtime online is different to empirical findings
% TODO JAMES only time training or also testing?

% TODO CHECK is "classification accuracy" correct?


% TODO NOW rename slice optimisation

% TODO CITE http://mlg.eng.cam.ac.uk/hoffmanm/papers/hoffman:2014.pdf
% TODO END remove todos from code
% TODO compare with other approaches (matern kernel)

% TODO roc_auc_score to score
% TODO END citations: switch to different format, make sure author is mentioned, If you use purely-numeric bibliographic references, do not forget to still mention authorsâ€™ surnames
% TODO convert sparse arff files to non sparse
% TODO rnd forest is bad at data thats heavily pos or neg, run tests
% TODO describe gradient problems (stress that the gradients are correct)
% TODO END random forest(s) spelling
% TOOD END dataset /data set spelling
% TODO END performance/time/accuracy, be clear
% TODO END run time spelling
% TODO END trade-off spelling
% TODO implement better optimiser based on gradients
% TODO comparing model evidence
% TODO http://www.springer.com/gp/book/9783540653677
% TODO END "n't" -> "not"
% TODO END don't talk about "subsection", subsub...
% TODO END emph new definitions
% TODO END use "equation" instead of equation* (also align)
% TODO END figure/Figure capitalisation
% TODO END capitalisation of chapters/sections
% TODO END no arxiv references
% TODO END present/past
% TODO UML diagram for schedulers
% TODO describe kernel problems
% TODO END check for emotional language
% TODO datasets: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
% TODO explain datasets w/ images if they are interesting
% TODO explain how exponential growth is at most 2x as bad as 100%
% TOOD the field of "sequential analysis"
% TODO metion bounds as alternative to EI
% TODO cite http://www.cs.ubc.ca/~poole/papers/randaccref.pdf
% TODO END no footnotes or cites after formulas
% TODO END random forest(s) consistent plural/capitalisatino (also logreg)

\documentclass[a4paper,12pt,twoside,openright]{report}

% Diagrams
\usepackage{graphicx}

\def\authorname{Jan S\"ondermann\xspace}
\def\authorcollege{Selwyn College\xspace}
\def\authoremail{jjes2@cam.ac.uk}
\def\dissertationtitle{Bayesian optimisation of approximateness in the trade-off between statistical and computational efficiency}
% TODO word count
\def\wordcount{@}

\usepackage{euscript}
\usepackage[export]{adjustbox}
\usepackage{subcaption}
\usepackage{chngpage}
\usepackage{calc}
\usepackage{epsfig,graphicx,parskip,setspace,tabularx,xspace} 
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\Break}{\State \textbf{break} }
\usepackage{listings}
\lstset{breaklines=true, frame=single, numbers=left, basicstyle=\small\ttfamily}

% TODO make this work
%\lstset{ %
%  commentstyle=\color{grey}    % comment style
%}


\DeclareMathOperator*{\argmax}{arg\,max}


%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

Our project falls into the field of meta-machine learning that tries to use machine learning methods to improve the result of using these methods.

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

%% START OF MAIN TEXT 

\chapter{Introduction}
% TODO talk more abt the statistics/compsci divide
% TODO add citations

\pagenumbering{arabic}
\setcounter{page}{1}
Before the advent of computers, statistical methods had to be simple enough to be calculable by hand which greatly limited the degree of complexity that these methods could reach. As computing power available to statisticians increased, many new methods were devised to make use of these new resources. In some cases, these new methods even exceed the computational possibilities and are computationally intractable. 

Although machine learning is a very diverse subject that combines influences from many different fields such as engineering, computer science, computational biology and physics, many of the recent advances have come from statistics or made use of statistical methods. As is true for mathematics more generally, considerations of run time and complexity are usually not in the focus of attention of statistical research. Public perception of machine learning has concentrated on the rapid growth of data to be analysed, a phenomenon often called "Big Data". This development has made issues of run time become increasingly acute. 

The current states of the field of machine learning requires skilled humans to make high level decisions on trade-offs between computational and statistical efficiency such as when to use approximate inference methods or when to use a simple method with lots of data rather than a complex method with a small amount of data. The knowledge necessary to make adequate decisions in these cases is often acquired through experience and an intuitive familiarity with the learning algorithms that can be hard to formalise and teach.

% cite auto. stat./auto weka
As part of the current trend to automise all aspects of machine learning, automating the process of finding a balance in these trade-offs would make an important contribution to the field. Finding this balance can be seen as an optimisation problem that optimises the function from a number of "approximation parameters" to the time and prediction performance of the learning algorithms. 

Classical optimisation methods may be ineffective as solutions to this problem as evaluating the function to be optimised can be very expensive to compute. Further complicating this problem is the fact that the time required to evaluate the function can vary drastically depending on where it is evaluated: consider the case where the approximation parameter is the number of layers in a deep neural network. Increasing the number of layers will cause the inference to take more time. The optimisation routine has to take this into account when exploring the function. A natural solution to this problem is Bayesian optimisation as Bayesian optimisation methods make very efficient use of data when optimising a function.

% TODO MAYBE cite http://rbr.cs.umass.edu/papers/ZCCijcai99.pdf
Adding run time considerations to statistical learning methods can lead to a number of different goals. One possible goal are anytime algorithms that can be interrupted while running, returning a solution optimal for the amount of time they were allowed to run for. Another goal is to develop contract algorithms that receive a data set and a time budget as input and infer an optimal model within the time budget.

% TODO is this true?
%In this project, we develop a system that @. As neither of these two problems has an analytic solutions, we wrote a number of heuristics that use Bayesian optimisation methods to decide @.

Our first major contribution in this project is a model of the function from approximation parameters to learning algorithm performance. This is the foundation for the second major part of our system which consists of a number of heuristics that use the model as a basis for Bayesian optimisation. Because these heuristics try to find an optimal evaluation sequence for the function to be optimised, we call them "schedulers".

\begin{figure}
\centering
  \includegraphics[trim=130 40 910 560,clip,width=\textwidth]{figures/anytime1.pdf}
  \caption{Comparing two schedulers}
  \label{anytime1}
\end{figure}

% TODO add . to prob
Figure \ref{anytime1} shows a plot that compares the performance of two such schedulers. The x-axis shows the time in seconds that the scheduler has been allowed to spend searching for the best model while the y-axis shows the classification accuracy of the best model the scheduler has found. The "\texttt{fixed\_exponential}" scheduler is a naive implementation that simply tries a fixed sequence of values for the approximation parameter. The "\texttt{EI * prob of success}" scheduler is a heuristic we developed to approximate an anytime algorithm. Both schedulers spend about 10 seconds in a burn-in phase. After this burn in ends, the blue scheduler immediately starts dominating the green scheduler and plateaus out at the maximum value about 50 seconds after learning starts. To reach the same classification accuracy, the naive scheduler needs about 80 seconds.

Although eventually both schedulers reach the same final accuracy, if we terminate both schedulers after 30 seconds, the best model found by the "\texttt{EI * prob of success}" is substantially better than that found by the naive scheduler at this point. It is therefore a heuristic for an anytime algorithm.

The rest of this dissertation is structured as follows:
\begin{itemize}
	\item The \textbf{Background} chapter introduces the theoretical ideas that this project is built on. It also lists the third-party technologies that were used during development
	\item The \textbf{Related Work} summarises the current state of research regarding the problems we are trying to solve
	\item The \textbf{Design and Implementation} chapter explains the features of our system, starting with a high level overview and continuing to a detailed description
	\item The \textbf{Evaluation} section critically examines what we have built by testing the system and evaluating results. It also lists challenges we faced and limitations of our current implementation
	\item The \textbf{Conclusion and future work} recapitulates our results by returning to the broader context lined out in this introduction and lists possible ways of extending the existing system
\end{itemize}












\chapter{Background}
% TODO A more extensive coverage of what's required to understand your work. In general you should assume the reader has a good undergraduate degree in computer science, but is not necessarily an expert in the particular area you've been working on. Hence this chapter may need to summarise some ``text book'' material. 
\textit{This chapter describes the background assumed in the remainder of the document. We give a brief explanation of general machine learning principles together with a summary of the algorithms used. We then describe the datasets we used during the course of the project.}





%\section{General machine Learning concepts}
%\begin{figure}
%\centering
%  \includegraphics[width=.8\textwidth]{figures/ml.pdf}
%  \caption{The general process of learning from data}
%  \label{mlstructure}
%\end{figure}

\section{Key concepts}
Our project falls into the field of meta machine learning. We use machine learning algorithms to learn and predict the performance of other machine learning algorithms. This makes it necessary to describe the 

% needs to understand ML. also a few concepts that we introduce in this project

% TODO ML overview
\subsection{Approximation parameters}
Approximation parameters are parameters to machine learning algorithms that influence the trade-off between statistical and computational efficiency. They let us vary the degree of approximateness at which the algorithm runs. Changing an approximation parameters should either decrease the run time at the cost of classification accuracy or improve accuracy while making training slower. 

The approximation parameter that we put most of our focus on in this project is the proportion of the available data. Other approximation parameters are hyperparameters to the machine learning algorithms such as the number of decision trees in a random forest. These parameters are considered in the modelling chapter to analyse how multiple interaction parameters interact.

Note that not all hyperparameters are approximation parameters. Some hyperparameters influence performance without changing the run time such as the length-scales of Gaussian process kernels, explained in depth in the Gaussian process section of this chapter.\footnote{Many machine learning packages include parameters such as the number of threads used for learning in their API. These are parameters that influence run time but not performance. They are, however, particular to the implementation and not part of the algorithm.}

% TODO why did you select the appr params you selected, later you only use data. data is enough to be interesting but we want to show that approach can in principle be extended

% TODO make a table that lists approx prarms and approx params that are 

\subsection{Scheduling}
As mentioned in the introduction, the problem we try to solve in this project is an optimisation problem with the additional component of running time constraints. A solution to this problem consists in devising strategies to decide on a sequence of evaluations of the function to be optimised such that we find an optimum within these constraints. 

We use the term "scheduler" to denote heuristics that decide on such a sequence. Schedulers allocate the time available for optimisation to the possible algorithm and hyperparameter combinations.

\section{Machine learning algorithms}
\subsection{Logistic regression and random forest}
% TODO mention that you focus on rnd-forest/log-reg and data only
This subsection introduces the two machine learning algorithms that we focus on in the evaluation of our system, random forest and logistic regression. Both of these algorithms are among the most common machine learning algorithm and are frequently used in real world applications. % cite

% TODO important to introduce algos to understand appr params
% TODO talk about what each algorithm is good/weak at

\subsubsection{Logistic regression}

\subsubsection{Random forests}
Random forests, introduced in \cite{rndforests}, is an algorithm that extends the idea of decision trees. Growing decision trees involves repeatedly splitting the feature space into rectangular regions using one feature, such that a the proportion of samples of the same class in the region is maximised \cite{james2014introduction}. 

% TODO what does high variance mean? applying multiple times to same data yields different results.
Individual decision trees alone can not compete with the predictive performance other algorithms, in particular they suffer from high variance. An initial solution to this, proposed in \cite{bagging}, is Bagging, a technique that creates a number of new training sets by drawing samples from the original dataset. The trees are then fitted to these new training sets. To make a prediction, the predictions of all trees are averaged over.

% TODO explain why this is done
Random forests improve the results obtained by bagging through decorrelating the trees. This is achieved by selecting a random subset of features from the full set of features at each training step. By combining and averaging over a set of decision trees as described, random forest achieves a performance that makes it a popular choice in machine learning.

The number of trees is a possible approximation parameter to random forests. Another hyperparameter that is also an approximation parameter is the minimum number of samples that must be contained in each region of the feature space.

\subsection{Gaussian Processes}
Gaussian processes are a machine learning method commonly used for regression problems. Unlike methods such as logistic regression or random forest that learn by updating a set of model parameters, Gaussian processes place a prior directly over the function $f$ \cite{Murphy:2012:MLP:2380985}. For this reason, they belong to the nonparametric family of learning algorithms. This prior then gets updated to a posterior based on the training data.

If we think of functions as infinitely sized vector indexed by the domain of the function, Gaussian processes can be understood as a generalisation of the multivariate Gaussian distribution to a distribution over infinitely many variables. Formally, a multivariate Gaussian distribution $\mathcal{N}(\mu, \Sigma)$ is specified by a mean vector $\mu$ and a covariance matrix $\Sigma$. Equivalently, a Gaussian process $\EuScript{G}\EuScript{P}(m, \kappa)$ is specified by a mean function $m(x)$ and a covariance function $\kappa(x, x')$ (if we think of single valued functions as infinite vectors then infinitely large matrices can be seen as functions with two arguments). It is common for the mean function to be a constant function with $\mu(x) = 0$ as the mean can be included in the covariance function. Covariance functions are also commonly called kernels. We will use the two terms interchangeably.

If we draw a function $f \thicksim \EuScript{G}\EuScript{P}(m, \kappa)$ from a Gaussian process, every finite set of inputs to the function $\mathbf{x} = (x_1, \cdots, x_n)$ span a multivariate Gaussian distributed vector $\mathbf{f} = (f(x_1), \cdots, f(x_n)) \thicksim \mathcal{N}(\mu,K)$ with $\mu = (m(x_1), \cdots, m(x_n))$ and $K_{ij} = \kappa(x_i, x_j)$ \cite{Rasmussen:2005:GPM:1162254}.

To express prior assumptions about the nature of the functions to be modelled, one has to choose a covariance function that expresses these assumptions by assigning appropriate covariances. There is a set of common covariance functions that Gaussian process libraries usually implement. In the remainder of this section, we explain how prior information is reflected in the kernels using the squared exponential covariance function as an example, following \cite{duvenaudthesis} in our explanation. We will return to this topic in the Design \& Implementation chapter when we describe the implementation of a custom covariance function. 

\subsubsection{The squared exponential kernel}
Probably the most widespread kernel in use is the squared exponential kernel. This kernel defines $\kappa(x, x') = \sigma_f^2 \text{exp}(-\frac{(x-x')^2}{2\ell^2})$ with $\sigma_f^2$ being the variance of the function and $\ell$ being the characteristic length-scale of the kernel. Figure \ref{sekernel} shows how the value of the squared exponential kernel changes as $x'$ moves away from $x$. 

The value of $\kappa(x, x')$ can be interpreted as the degree of similarity between $f(x)$ and $f(x')$ with larger values of $\kappa$ meaning higher similarity. This means that the squared exponential kernel expresses the prior that the functions the Gaussian process will model will be smooth. Precisely how smooth they will be is expressed by the length-scale $\ell$. This is a hyperparameter to the squared exponential kernel which allows us to separate general assumptions, such as "the functions will be smooth", from specific models. Figure \ref{sekernel_ls} shows how changing the value of $\ell$ influences the shape of the kernel function.

\begin{figure}
\centering
  \includegraphics[trim=120 240 100 230,clip,width=.4\textwidth]{figures/se_kernel.pdf}
  \caption{The squared exponential kernel}
  \label{sekernel}
\end{figure}

\begin{figure}
\centering
  \includegraphics[trim=120 240 100 230,clip,width=.4\textwidth]{figures/se_kernel_different_ls.pdf}
  \caption{Three different values for $\ell$ in the squared exponential kernel}
  \label{sekernel_ls}
\end{figure}

Figure \ref{sekernel_draws} shows five functions drawn from a Gaussian process with a squared exponential kernel with $\sigma_f^2 = \ell = 1$. Note how although the functions take different value, they share the same degree of smoothness and range. Figure \ref{sekernel_draws_different_ls} shows the effect that varying $\ell$ has on the functions: for small values, the functions become rugged.


\begin{figure}
\centering
  \includegraphics[trim=120 240 100 230,clip,width=.4\textwidth]{figures/se_kernel_draws.pdf}
  \caption{Five draws from a Gaussian process with a squared exponential kernel}
  \label{sekernel_draws}
\end{figure}

\begin{figure}
\centering
  \includegraphics[trim=120 240 100 230,clip,width=.4\textwidth]{figures/se_kernel_draws_different_ls.pdf}
  \caption{Functions drawn from Gaussian processes with squared exponential kernels that have different length-scales}
  \label{sekernel_draws_different_ls}
\end{figure}

The squared exponential kernel is an example of a class of kernels called \emph{stationary} that have the property that their value does not change if $x$ and $x'$ are shifted by the same amount. Formally, for stationary kernels, $\kappa(x, x') = \kappa(\tau + x, \tau + x')$. 


\subsubsection{The linear kernel}
The linear kernel is defined as $\kappa(x, x') = \sigma_f^2(x-c)(x'-c)$ with $c$ determining the $x$-coordinate of the point that all the functions in the posterior pass through \cite{duvenaudthesis}. The linear kernel is an example of a kernel that is not stationary as it models global, linear change in the functions.


\subsubsection{Combining kernels}
Although there is a substantial number of such common kernels, their number is still finite. To express more complex priors, it is therefore necessary to combine existing kernels to form new ones. One way to do so is to add them together. Figure \ref{sum_of_lin_and_se} shows functions drawn from a Gaussian process that has as its kernel the sum of a squared exponential and a linear kernel. It combines a linear component with the noisiness of the squared exponential kernel.

\begin{figure}
\centering
  \includegraphics[trim=80 210 70 195,clip,width=.4\textwidth]{figures/sum_of_lin_and_se.pdf}
  \caption{Adding a linear kernel to a squared exponential kernel}
  \label{sum_of_lin_and_se}
\end{figure}

Another way to combine kernels is multiplication. Figure \ref{prod_of_lin_and_lin} shows the result of multiplying to linear kernels together, which results in quadratic functions.

\begin{figure}
\centering
  \includegraphics[trim=80 210 70 195,clip,width=.4\textwidth]{figures/prod_of_lin_and_lin.pdf}
  \caption{Multiplying two linear kernels}
  \label{prod_of_lin_and_lin}
\end{figure}



\section{Bayesian optimisation and expected improvement}
% TODO consistent naming for "prob of impr" and EI

Bayesian optimisation is a method to solve the optimisation problem of having to find an $x$ that maximises\footnote{We will consider maximisation only in this section as minimising $f$ is equivalent to maximising $-f$} $f(x)$ for a given function $f$, commonly written 
\begin{equation}
\argmax_x f(x)
\end{equation}

What distinguishes Bayesian optimisation from other optimisation methods is that it places a prior over the function $f$. After evaluating $f$ and obtaining a new data point, if updates this prior and uses the new posterior to decide where to evaluate the function next.

As Gaussian processes are precisely such priors over functions, they are very well suited to be used with Bayesian optimisation. By selecting an appropriate kernel, one can express existing prior knowledge over the function to be optimised which is crucial in ensuring that the Bayesian optimiser makes good choices in deciding the sequence of function evaluations.

Unlike other optimisation methods like gradient descent that only use information local to the function at the last evaluation, Bayesian optimisation considers all the available data when deciding where to evaluate the function next. This means that Bayesian optimisation often finds an optimum in fewer steps than other methods. This, however, comes at the cost of requiring more computational resources when making these decisions \cite{PracticalBayesianOptimization}.

This means that Bayesian optimisation is especially well suited in cases like ours where evaluating $f$ is (potentially) very costly and the number of function evaluation should be minimised. In Addition, Bayesian optimisation has shown to be very successful at optimising hyperparameters of machine learning algorithms \cite{PracticalBayesianOptimization}. This is a problem that shares many similarities with the one our project addresses. We will return to Bayesian optimisation in the Evaluation chapter when we consider whether it was the right choice for our project.

Besides the function prior, one also needs to choose an acquisition function when using Bayesian optimisation. These function take the posterior over $f$ as input and return a function $a$. This function is used to choose the next input value to $f$ with a proxy optimisation, which means that the optimum of $a$ is also the point where we next evaluate $f$. This naturally requires $a$ to be quick to evaluate.

Out of the possible acquisition functions, we consider two that we use in the implementation of our schedulers: Probability of improvement and Expected improvement. Figure \ref{bayesianopti} shows an example of an optimisation in progress. The topmost plot shows the function $f$ as a dashed, blue line. This is the true, underlying function to which we only have access through individual evaluations. It function has been evaluated seven times and the Gaussian process has created a model with the mean shown as a read line and twice the standard deviation shown in grey. This is the information that is available to us when we construct the acquisition function.

\begin{figure}
\centering
  \includegraphics[trim=60 120 60 120,clip,width=\textwidth]{figures/bayesian_opti.pdf}
  \caption{Two different acquisition functions for Bayesian optimisation}
  \label{bayesianopti}
\end{figure}

The second graph shows the probability of improvement acquisition function. This acquisition function is based on the strategy of finding the point most likely to yield an improvement over the current maximum $y_{max}$. It is calculated with
\begin{align}
a_{PI}(x) &= P(z \geq y_{max}) \text{\ with\ } z \thicksim \mathcal{N}(\mu(x),\sigma^2(x))\label{eq:pi_equation}\\
&=\Phi(\frac{\mu(x) - y_{max}}{\sigma(x)})
\end{align}

% TODO cite kushner 1964
% TODO http://mlss2014.com/files/defreitas_slides1.pdf
Equation \ref{eq:pi_equation} is visualised in figure @. Due to the fact that the Gaussian process tends to be most certain about points that are close to a known data point and the probability of improvement strategy does not take into account the magnitude of the difference between the function value at $x$ and the current maximum, thir tends to be very conservative acquisition function. In figure \ref{bayesianopti}, we can see the acquisition function for the probability of improvement in the second plot. Note how $a_{PI}$ is greatest at an $x$ value of around 4, close to where we have evaluated the function, and quickly drops off as $x$ increases. This is the phenomenon just described: this acquisition strategy is certain that by making a very small step away from the existing data point in a direction where the mean increases, it can improve the current maximum.

% TODO cite mockus 1978, better place to cite
Another, superior, strategy to decide which point to evaluate next is Expected improvement. The acquisition function for Expected improvement is defined \cite{eipaper} as
\begin{align}
a_{EI}(x) &= \mathbb{E}(\text{max}\{z - y_{max}, 0\}) \text{\ with\ } z \thicksim \mathcal{N}(\mu(x),\sigma^2(x))\\
&= (\mu(x) - f_{max})\Phi(\frac{\mu(x) - y_{max}}{\sigma(x)}) + \sigma(x)(\frac{\mu(x) - y_{max}}{\sigma(x)})
\end{align}

% TODO give an intuition for what this means
% TODO look up what max does to an expectation

% TODO bad style (we)
If we look at the third plot in figure \ref{bayesianopti}, we can see the difference between probability and improvement and Expected improvement in the range between x values four and five. In contrast to probability of improvement, the acquisition function for expected improvement has its maximum further to the right, as it also considers the difference between the current maximum and the y value it expects. 






\section{Third-party libraries used}
Besides the standard libraries of Python 2.7 and Matlab 2014b, our project uses a number of third-party libraries which are described in this section. The first and most important library that we make extensive us of is scikit-learn, a Python implementation of many common machine learning algorithms. Scikit-learn is built on NumPy and SciPy, two very widespread Python frameworks for scientific computing. Besides its implementation of random forest and logistic regression, we also use it to generate datasets with its \texttt{make\_classification} function as explained in the previous section.

We further use the "Gaussian Processes for Machine Learning" (GPML) Matlab library by Rasmussen and Williams, a Gaussian Process framework. Although there are Gaussian process implementations in Python such as GPy, the maturity of GPML and our experience in using it made us choose GPML over its alternatives. One compelling advantage of GPML is that it is very straightforward to implement new kernels which we had to do for this project.

Finally, we use jsonlab, a json implementation for Matlab and matplotlib, a Python clone of Matlab's plotting functions.



\chapter{Related Work}
Jordan \cite{jordan2013}

While little work has been published on the precise problem that we are considering in this report, there has been a strong interest in the automatic selection of hyperparameters.

In \cite{ThoHutHooLey13-AutoWEKA}, the authors build on the popular WEKA framework, an implementation of many machine learning algorithms in Java, to create a system that automatically chooses and algorithm together with appropriate hyperparameters. % TODO bayesian opt




% TODO Auto weka
% TODO look at spearmint

%- hyper param optim
%- multi armed bandits



% TODO This chapter covers relevant (and typically, recent) research which you build upon (or improve upon). There are two complementary goals for this chapter: - to show that you know and understand the state of the art; and to put your work in context. Ideally you can tackle both together by providing a critique of related work, and describing what is insufficient (and how you do better!)




% TODO not "score"
\chapter{Modelling Learning Time and Score}
To make good decisions, the scheduler needs an accurate model of the dependance of approximation parameters and performance. Creating such models is therefore a fundamental piece in our system.

\section{Collecting Sample Data}
The first step towards modelling performance was to collect sample data. This data was used to first investigate the function and formulate a hypothesis on the character of the function from approximation parameters to performance. After the kernel was implemented, we tested it by comparing its predictions based on the sample data with possible alternative kernels. The details of this are described in the Evaluation chapter below.

% TODO MAYBE don't call this a script
\subsection{Data collection script}
As data collection was resource and time intensive, we implemented a command line script that accepts a range of configuration flags and collects data based on the parameters it receives. This script was then executed on an Amazon EC2 instance. This script can be configured with the following list of command line flags:

\begin{itemize}
\item \texttt{-a/--algorithm} The machine learning algorithm to be run. This can be one of \texttt{rnd\_forest}, \texttt{log\_reg} or \texttt{svm}
\item Exactly one out of the following ways to load data:
\begin{itemize}[label=$\star$]
        \item \texttt{-s/--synthetic} Create synthetic data using scikit-learn's \texttt{make\_classification}. The parameters to \texttt{make\_classification} should be specified in string containing a python dictionary (e.g. \texttt{"{'n\_samples': 5000}"})
        \item \texttt{-l/--load-arff} Load one of the \texttt{arff} files in the \texttt{data} directory
        \item \texttt{-z/--datasets-of-size} This loads all the datasets of the given size. Can be one of \texttt{small}, \texttt{medium} or \texttt{large}
        \item Vector Space Models
     \end{itemize}
\item \texttt{-d/--percentage-data-values} An array of data percentages specified in the syntax explained below
\item Any additional parameters in the form \texttt{parameter\_name:[int|float]-<array of values as explained below>}
\item \texttt{-p/--parallel} This flag was originally included to parallelise data collection by using multiple threads. Testing it revealed that running multiple instances of the algorithms in parallel led to distorted running times. This flag was not used for data collection
\end{itemize}

% TODO are these really sequences?
The syntax to specify arrays in the command line arguments allows expressing arithmetic and geometric sequences. Arithmetic sequences are created with \texttt{a:min:length:max}, e.g. \texttt{a:1:4:50} to create an array of four evenly spaced elements with the first being 1 and the last being 50. The syntax to create geometric sequences is \texttt{g:min:length:max:base}. It includes the base of the geometric sequence. In this syntax, the array \texttt{[2, 4, 8, 16, 32, 64]} is expressed as \texttt{g:2:6:64:2}. Avoiding hard coded values for data generation in this fashion made it possible to quickly collect fresh data.
	
When collecting data, the program takes the cartesian product of these arrays and runs the learning algorithm once for every element in the product. This number grows exponentially with every parameter which made access to the EC2 instance, which could be run overnight, crucial.

Once it has finished all the computations, the program outputs the data to a comma-separated value \texttt{csv} file with a column containing a unique id for the dataset, one column for each parameter and one column each for the percentage of data used, the time spent on learning and the classification accuracy. All the values in these files are numbers which makes it easy to import them into Matlab as matrices. 

\subsection{Collected sample data}

% TODO JAMES maybe use different figures
\begin{figure}
\centering
  \includegraphics[trim=50 200 35 205,clip,width=.6\linewidth]{figures/lr_mnist.pdf}
  \caption{Running logistic regression on the MNIST dataset}
  \label{sampledata1}
\end{figure}

\begin{figure}
\centering
  \includegraphics[trim=50 200 35 205,clip,width=.6\linewidth]{figures/rf_mnist.pdf}
  \caption{Running random forest on the MNIST dataset}
  \label{sampledata2}
\end{figure}

\begin{figure}
\centering
  \includegraphics[trim=50 200 35 205,clip,width=.6\linewidth]{figures/lr_synth.pdf}
  \caption{Running logistic regression on a synthetic dataset}
  \label{sampledata3}
\end{figure}

\begin{figure}
\centering
  \includegraphics[trim=50 200 35 205,clip,width=.6\linewidth]{figures/rf_synth.pdf}
  \caption{Running random forest on a synthetic dataset}
  \label{sampledata4}
\end{figure}



% TODO why suddenly misclassification instead of accuracy, explain that one is 1-the other
Figures \ref{sampledata1}, \ref{sampledata2}, \ref{sampledata3} and \ref{sampledata4} show the results of collecting performance data, varying the percentage of data used. The four plots show the result of running random forest with 128 trees and logistic regression on the MNIST dataset and a synthetic dataset with 5000 samples and 500 features.

In all four figures, the run time shown in blue shows linear growth as the amount of data increases. As run time values shown in figure \ref{sampledata3} are substantially lower, the values are noisier than in the other graphs, which is to be expected. The run time shown in figure \ref{sampledata2} shows a change in growth at around 75\%. This is an effect which sometimes occurred during data collection and which we analyse in detail in the Challenges section of the Evaluation chapter.

The error rates for all four combinations of algorithms and datasets shown in red exhibits exponentially decaying behaviour, falling very rapidly initially before levelling off.

% TODO also show plot for # trees (or another appr param)








\section{The Exponential Mixture Kernel}
% TODO change k to \kappa
% TODO use x, x' and t, t' consistently 

Based on the results shown in the previous section, we implemented a kernel to model the exponential behaviour of classification accuracy. Following \cite{2014arXiv1406.3896S}, we define the kernel as
% TODO JAMES can this be written as \psi(\lambda)d\lambda ?
\begin{align}
k(t,t') &= \int_{0}^{\infty} e^{-\lambda t}e^{-\lambda t'}\mu(d\lambda)\\
&= \int_{0}^{\infty} e^{-\lambda(t+t')}\mu(d\lambda)
\end{align}

% TODO kernel assumptions: decay starts at zero, maybe add limes
% TODO does this kernel always go to 0?
with $\mu$ being a mixing measure\footnote{We use $\mu$ instead of $\psi$ to denote the mixing measure to avoid confusion with the $\psi$ parameter to the gamma function explained below.}that weighs the $e^{-\lambda(t+t')}$ term. Note that this is not a stationary kernel as $k(t, t') \neq k(a + t, a + t')$. This conforms to our prior that moving further away from the origin, the value of $k$ should decrease.

Again following \cite{2014arXiv1406.3896S}, we choose a gamma distribution as $\mu$ which leads to an analytic solution to the integral:
\begin{align}
k(t, t') &= \int_0^{\infty} e^{-\lambda(t+t')}\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-\lambda\beta} d\lambda\\
&=\frac{\beta^\alpha}{\Gamma(\alpha)}\int_0^\infty e^{-\lambda(t+t'+\beta)}\lambda^{\alpha-1}d\lambda\\
&=\frac{\beta^\alpha}{(t+t'+\beta)^\alpha}
\end{align}

Diverging from \cite{2014arXiv1406.3896S}, we reparameterise the gamma distribution with
\begin{equation}
\psi = \mathbb{E}(x) = \frac{\alpha}{\beta}
\end{equation}
and
\begin{align}
\xi &= \frac{Var(x)}{\mathbb{E}^2 (x)} = \frac{\alpha}{\beta^2} \cdot \frac{\beta^2}{\alpha^2}\\
&= \frac{1}{\alpha}
\end{align}

such that the term for our kernel becomes
\begin{equation}
\kappa(x, x') = \frac{\frac{1}{\psi\xi}^{\frac{1}{\xi}}}{(x+x'+\frac{1}{\psi\xi})^{\frac{1}{\xi}}}
\end{equation}

% TODO show plots of reparameterised gamma function
% TODO show plots drawn from priors

% TODO show derivatives

\section{Selecting Kernel Hyperparameters} 
% TODO JAMES is it correct to call this model selection?

To fit a Gaussian process to a given set of data points, one needs to find appropriate values for the hyperparameters to its covariance and likelihood function. There two main methods to achieve this are marginalisation and optimisation. For reasons explained in the evaluation chapter, we implement both of these methods in our project.

% TODO this algorithm is "too clever"
% TODO show figs where optimisation fails
GPML comes with an optimisation function that is meant to be used to optimise hyperparameters. It relies on gradient information which we implemented for the exponential mixture kernel. We were, however, not able to use this function with our kernel as it would terminate after a very small number of steps having barely moved from the initial values without having found a minimum. We suspect the reason for this to be numerical instability that creates very small local minima that the optimiser is unable to escape from. Figure @ shows an example of a marginal likelihood function that we tried to optimise with GPML's \texttt{minimize}.

After failing to optimise the hyperparameters to our kernel using the optimisation function included with GPML, we decided to implement sampling which does not suffer from the same problems as optimisation. For reasons of speed and ease of use, we then decided to implement our own optimisation routine which is described in detail below. An evaluation of the two methods applied to our data can be found in the Evaluation chapter.


\subsection{Sampling}
The first approach to hyperparameter selection we use in our project is the Bayesian approach of integrating out the hyperparameters $\theta$:

% TODO maybe murphy equation 15.1
\begin{equation}
p(y|X) = \int p(y|X,\theta)p(\theta)d\theta\\
\end{equation}

We decided to implement slice sampling
decided to implement this when gpml minimise didn't work

The main advantage of sampling the hyperparameters is that it does not suffer from overfitting the same way optimisation does, given that the samples are successfully drawn from the posterior distribution. Its major drawback is that it is computationally more expensive than optimisation methods. Implementation wise, sampling also takes more effort than optimisation, as the result of the computations the program executes have to be correctly averaged over.


%The Bayesian way to handle hyperparameters is to integrate them out. 
% TODO add maths on integrating out hyperparams

%To approximate this integral, MCMC methods are
% TODO maths on MCMC

%To sample from @ we use slice sampling. adv/disadv: \cite{neal2003}
% TODO why slice sampling and not another method
%- mcmc/sampling



\subsection{Optimisation}
Another approach to find suitable hyperparameters is to optimise the marginal likelihood of the model with respect to its hyperparameters. This has the advantage of being substantially faster than sampling. It is also easier to implement, as it returns one set of hyperparameters that can be used to make predictions instead of multiple samples that have to be correctly averaged over.

The great disadvantage of optimisation is that it is prone to overfitting by choosing one optimum in cases where multiple viable optima exist. We describe situations in which we encountered this problem in the Evaluation chapter, comparing it with the results of slice sampling.



After inspecting the marginal likelihood function @of sample data@ in Matlab, we concluded that it is not a function that is inherently difficult to optimise and decided to implement our own optimisation routine. As we had already successfully sampled hyperparameters using slice sampling as described above, we implemented an optimisation method that works similarly to how slice sampling draws samples. The first version of this algorithm is shown below as Algorithm \ref{opti1}.


\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Slice\_optimisation}{$f,x,iterations=100,width=1$}
\State $y\gets f(x),\ D\gets \Call{get\_dimensions}{x}$
\For{$i\gets 1, iterations$}
\For{$dim\gets \Call{permute}{D}$}\Comment{Iterate over all dimensions}
\State $x_l, x_r, x'\gets x$\Comment{$x_l,\ x_r$ span interval, $x'$ falls inside}
\State $r\gets \Call{Uniform}{0, 1}$
\State $x_l(dim)\gets x(dim) - r * width$
\State $x_r(dim)\gets x(dim) + (1 - r) * width$
\For{$j\gets 1, 15$}
\State $x'(dim)\gets \Call{Uniform}{x_r(dim), x_l(dim)}$
\State $y' = f(x')$
\If{$y' < y$}
\State $y\gets y',\ x(dim) = x'(dim)$\Comment{New optimum}
\Break
\EndIf
\If{$x'(dim) > x(dim)$}
\State $x_r(dim) = x'(dim)$\Comment{Narrow interval from the right}
\ElsIf{$x'(dim < x(dim)$}
\State $x_l(dim) = x'(dim)$\Comment{Narrow interval from the left}
\EndIf
\EndFor
\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\caption{First version of slice optimisation}
\label{opti1}
\end{algorithm}

This optimisation algorithm starts at a given $x$ and optimises by iterating over the input dimensions, spanning up an interval around $x$ for every iteration. Inside of these iterations, it selects points $x'$ at random from the interval. If $f(x')$ is smaller than the current minimum, it continues to the next loop iteration, otherwise it narrows the interval either from the left or the right, depending on the side of $x$ on which $x'$ falls.

Given enough data, this algorithm is able to find suitable hyperparameters to the exponential mixture kernel reliably. If it is used to fit a GP to a small number of points (five or less), it often selects hyperparameters such that the covariance matrix is not positive semidefinite and Cholesky decomposition fails. We catch these errors by wrapping every evaluation of $f$ in a \texttt{try ... catch} block. 

While testing this algorithm, we found an effective method to handle these errors to be rerunning the algorithm multiple times and selecting the result with the highest marginal likelihood. This also alleviates the issue of local optima which we discuss in detail in the Evaluation chapter. The code to achieve this is shown as Algorithm \ref{optiwrap}.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Optimise\_with\_restarts}{$f,x,restarts=5$}
\State $X = [\ ],\ Y = [\ ]$
\For{$i\gets 1, restarts$}
\State $x'\gets \Call{Slice\_optimisation}{f, x}$
\State $y'\gets f(x')$
\State $\Call{append}{X, x'}$
\State $\Call{append}{Y, y'}$
\EndFor
\State $i\gets \Call{MinIndex}{Y}$
\State \textbf{return} $X(i)$
\EndProcedure
\end{algorithmic}
\caption{Rerunning the optimiser}
\label{optiwrap}
\end{algorithm}

% TODO stress that this is only important for development and that when using actual data, this would get dominated (in the evaluation chapter)
While this updated algorithm produces adequate results, it takes a substantial amount of time to optimise. A final change we therefore added was to terminate the optimisation routine early if the value of $y$ only changes minimally between five optimisations. This condition is met during almost all iterations and often drastically cuts short the time that our algorithm needs.

% TODO MAYBE mention that there is an implementation of it, what did you try to make it work?
One shortcoming of this optimisation routine is that all its steps have to be axis aligned. If the gradient of the function being optimised at $x$ is not aligned with any axis, this causes the optimiser to make very small steps along multiple axes. A superior approach would be to directly move along the gradients, which are available to us. As our current algorithm already performs to a high standard, we did not investigate this potential improvement further.


% TODO mention that the result of optimisation is handled as one sample, averaged over one


%show diagrams, talk about the relationship. is it always linear for time (no, svm, O(n^2-3)), is it always exponential for score

\subsection{Using both methods} % TODO different name

As our program has to be able to handle both samples and single results returned by optimisation, we decided to treat the result of our optimisation routine as a single sample in the routines that make use of our model. This makes it completely transparent to these routines which of the two hyperparameter selection methods were used and enables us to switch between them according to the requirements of the schedulers.

% TODO END hyperparameter spelling


% TODO "classification accuracy" maybe isn't correct if we also use roc
% TODO mention somewhere that "performance" means time + score
\chapter{Scheduling} 

% TODO explain burn in and how you don't model during burn in

\textit{This chapter describes the theoretical and practical aspects system we developed. It firsts gives a high level overview of the system architecture before describing in detail how the scheduling heuristics try to find optima within the time constraints.}

\section{System Architecture} % TODO maybe something like "overview"

% TODO explain how every scheduler optimises over multiple algorithms, each algorithm has its own acquisition function
% TODO explain burn in better
% TODO mention drawing
% TODO mention how multiple schedulers can be executed simultaneously
% TODO include and explain figure  of two schedulers running at the same time
% TODO go more into details of what schedulers do


\subsection{High level architecture}
% TODO END make sure this is properly centered
\begin{figure}[!ht]
  \begin{adjustwidth}{-\oddsidemargin-2in}{-\rightmargin-1.5in}
    \centering
    \includegraphics[trim=0 0 0 0,clip,width=0.9\paperwidth]{figures/architecture.pdf}
    
  \end{adjustwidth}
  \caption{High level architecture, shown after three iterations. Data is coloured red, machine learning algorithms are coloured blue}
    \label{architecture}
\end{figure}

Our system is designed in a two tiered fashion. Tier 1 creates models of the data using learning algorithms that are parameterised with approximation parameters. The training time and prediction accuracy of these models makes up the input to tier 2. This second layer first creates a meta-model of the model performances of the models in tier 1. This meta-model is then used by the scheduling part of the program to select the algorithm and approximation parameters for the next iteration. 

Figure \ref{architecture} shows this architecture diagrammatically after three iterations. Note how both tiers execute machine learning algorithms (coloured in blue) on a set of data (coloured in red) and how the result of the algorithms in the first tier form the data set for the Gaussian Process at the top of the diagram. The main loop of our program is shown, slightly simplified, in Figure \ref{mainloop}. When executing, the program switches back and forth between the two tiers: it executes an algorithm with a given set of approximation parameters, builds a new model that includes the performance during this execution. The scheduler then decides which algorithm/approximation parameter combination to run next and the system returns to the first step.

% TODO mention that all the performances are recorded and new datapoints are appended

\begin{figure}[ht]
\begin{lstlisting}[language=Python]
while True:
   scheduler.decide() # Tier 2 (scheduling)
   if scheduler.decision:
      scheduler.execute() # Tier 1
      scheduler.model() # Tier 2 (modelling)
\end{lstlisting}
\caption{The main loop}
\label{mainloop}
\end{figure}


\subsection{Directory structure of the code}

The directory of the project contains the following subdirectories:
\begin{itemize}
\item \textbf{data/}: This directory contains datasets in \texttt{data/raw\_arffs/}. It is also where data generated by the data generation scripts are stored. When creating plots based on this data, the results are written into this folder
\item \textbf{figs/}: Every diagram that our system generates is automatically also saved to this directory
\item \textbf{report}/: This folder is where the source file and diagrams for this document are stored
\item \textbf{src/}: This directory holds all the source code for our project. It contains \texttt{main.py}, the main code file and two subdirectories, \texttt{src/data\_handling/} and \texttt{src/matlab/} which contain Python and Matlab code respectively
\item \textbf{var/}: This folder is used to store temporary files that are used to exchange data between Python and Matlab code
\end{itemize}

\subsection{Interoperability between Python and Matlab}
Choosing Matlab to implement the performance modelling code and Python to implement the other parts of our system made it necessary to devise a method of transferring data between these two parts of the program. During the initial planning stage, we intended to write all parts of the system in Python but the advantages of GPML such as level of documentation and ease of use later turned out to outweigh the disadvantages of dealing with process interoperability.

To start the program, the \texttt{main.py} file is executed. This file then periodically calls a Matlab script whenever it has collected new data and needs to update one of its performance models. Data is transferred between the Python and the Matlab scripts by writing it to \texttt{json} files\footnote{JSON is a widespread data format similar to XML.}. Before this script is executed, \texttt{main.py} writes the recorded performances of the algorithm for which it has just updated its data to \texttt{var/scheduler\_data.json}. This file contains a JSON object with the fields \texttt{x\_percent\_data}, \texttt{y\_times} and \texttt{y\_scores} which hold the data collected so far.

Once it has finished modelling, the Matlab script writes the models to \texttt{var/models.json} and terminates. This file contains an array of model objects. These models are represented as 100 equally spaced data points between 0 and 100 \%. They each have an \texttt{m} and an \texttt{sd} field set for mean and standard deviation respectively. This file is then read by \texttt{main.py} and the old models are overwritten with the updated values.


% TODO move this up to the prev chapter
\section{Measuring performance}
% TODO END Cross-Validation spelling
As shown in figure \ref{architecture}, the output of tier 1 is the performance of the model that the learning algorithm creates of the dataset given the particular approximation parameters that are selected by the scheduler. This section explains how the performance of the models is measured. Specifically, we describe how we measure predictive accuracy and training time of our models.

\subsection{Classification accuracy}

Measuring classification accuracy is more complex than stopping time. One common approach to measure the performance of a model is to split the dataset into two subsets, train the model on one subset and compare its predictions on the second subset with the actual values. This method has the disadvantage that only a subset of the available data is used for training which can make it susceptible to variance in reported classification accuracy depending on how the data is split.

A superior method, that we slightly adjust to the requirements of this project is k-Fold Cross-Validation. This approach to measure model performance splits the dataset into $k$ subsets of equal size, here called folds. The algorithm then learns on the combined data of the second to $k$th folds, using the first fold as a validation set. This process is repeated $k$-times, using another fold as validation set in each iteration. The classification accuracies of the $k$ iterations are then averaged to calculate the overall classification accuracy. This approach ensures that all the data is used for training. Although the learning algorithm has to be executed $k$ times instead of just one, $k$ is a constant value in k-Fold Cross-Validation that does not depend on the size of the dataset and therefore does not impact the asymptotic run time.

To implement the percentage of data approximation parameter, we need to restrict the data that is used for training our models while still keeping the advantages of k-fold cross-validation. We achieve this by further splitting the folds used for training. For each fold, a subset of samples is selected according to the percentage of data to be used. The fold used for validation, however, does not change. This approach ensures that all available data is used for validation.

The classification accuracy in cases where the samples belong to more than one class is calculated as the percentage of samples whose class was correctly predicted. Using two different scoring algorithms is valid because we never compare performance across datasets.

% TODO add diagram, give better intuition
When calculating the score that a model achieves on a given validation set, two different methods are used. For binary classification tasks\footnote{Those in which each sample belongs to one of two classes such as \texttt{true} and \texttt{false}.}, we use the receiver operating characteristic (ROC) metric to asses model accuracy. This metric calculates the percentage of the area under the curve that plots the false positive rate of the classifier against its true positive rate. This metric has been shown to be more robust than the simpler accuracy metric we use in the multiclass case \cite{Bradley97theuse}. Calculating an ROC curve is only possible for binary classification tasks because true and false positive rates are not defined in this case.

\subsection{Time}
Measuring the time spent learning is more straightforward than measuring classification accuracy. We use Python's \texttt{time.time()} function to stop the time that elapses while training. As we don't want to include validation time spent in the iterations of Cross-Validation, we keep the current time spent in a variable that gets updated after training in each iteration finishes.

Another factor to be considered is that other tasks running in parallel can pollute the timing data. During development of the program, we were on occasion forced to shut down resource intensive programs while running tests. This problem did not occur on the EC2 instance. % TODO why not?


% TODO explain why this doesn't underestimate performance by not using all the data

Before we split the dataset into the ten folds, we shuffle the samples to ensure proper randomisation across the folds. 






% TODO END don't just use "algorithms", use learning algorithms. also "performance model", not just "model"







\section{Implemented Schedulers}
Every scheduler is implemented in a separate class of which there are nine in total. There are three abstract scheduler classes that implement functionality shared between two or more schedulers while the other six implement scheduling strategies that can be used to schedule data collection. In this section, we describe each scheduler's functionality before comparing and evaluating them in the Evaluation chapter.

\subsection{\texttt{\textit{Scheduler}}}
This is the base class that all schedulers inherit from. It defines a framework general enough that all schedulers implement it. Besides drawing routines, which are mostly shared among schedulers, and code to write performance data and read model data, this class contains the following methods:

\begin{itemize}
\item \texttt{\_\_init\_\_}: the class constructer initialises a number of properties, including the \texttt{self.data} property which holds an array of learning performances for each algorithm
\item \texttt{decide}: this is a virtual method that is called to decide on the next algorithm/approximation parameter combination to be evaluated. It sets the \texttt{self.decision} property of the scheduler object. If the scheduler is done, either due to having finished optimising or because its time budget has been used up, this property is set to \texttt{None}
\item \texttt{execute}: this method executes the decision made by the \texttt{decide} method and adds the new time and score to the \texttt{self.data} property
\item \texttt{model}: this function updates the model for the algorithm for which data was collected during the last execution of the \texttt{execute} method. It reads the value of the \texttt{self.decision} property to determine the algorithm and is to be called after \texttt{execute} finishes
\end{itemize}

% TODO MAYBE explain how drawing is spread over several methods

\subsection{\texttt{FixedSequenceScheduler}}
\texttt{FixedSequenceScheduler} is the simplest of all the scheduling strategies implemented. Its constructor takes a sequence of data percentage values and computes the cartesian product of the learning algorithms and this sequence and saves this list. When its \texttt{decide} method is called, it writes the next element of the list into \texttt{self.decision}, cycling through algorithms and executing the fixed sequence of data percentage values.

% TODO figure
% TODO END percentage of data values, consistent term

\subsection{\texttt{\textit{ProbablisticScheduler}}}
This is a base class of all schedulers that make probabilistic decisions using an acquisition function as described in the Bayesian optimisation section in the Background chapter. It has a virtual class method \texttt{a} that takes a time and score model (expressed as arrays of mean and standard deviation) and calculates the acquisition function as array.

If slice sampling was used to integrate out the hyperparameters to the exponential mixture kernel, \texttt{\textit{ProbablisticScheduler}} is responsible for calling \texttt{a} as many times as there are samples and averaging over these acquisition functions. Time and score model samples are taking independently which allows us to randomly pair one time and one score model per iteration.

\subsection{\texttt{ProbabilityOfImprovementScheduler}}
This is the simplest concrete probabilistic scheduler. It implements the probability of improvement heuristic for Bayesian optimisation. As mentioned in the background chapter, this is not a competitive scheduling strategy but is still included for purposes of comparison.

This scheduler truncates its acquisition function at the maximum x value for each algorithm and only considers percentage of data values that are strictly greater than its current maximum. This encodes the assumption that using more data can never be worse than the current best result.

Figure @ shows the \texttt{ProbabilityOfImprovementScheduler} after a few iterations. Note how its acquisition function quickly drops off as uncertainty grows.

\subsection{\texttt{ExpectedImprovementScheduler}}
\texttt{ExpectedImprovementScheduler} implements the Expected improvement heuristics for Bayesian optimisation. Like the \texttt{ProbabilityOfImprovementScheduler}, it truncates its acquisition function and only considers using more data than before for subsequent execution of the learning algorithms.

\subsection{\texttt{ExpectedImprovementPerTimeScheduler}}
This scheduler inherits from the \texttt{ExpectedImprovementScheduler} and differs from it only by dividing the expected improvement by the mean of the time model. This biases it towards faster executions of the learning algorithms, even if the improvement over the current best is not as large as slower executions.

\subsection{\texttt{\textit{TimedScheduler}}}
\subsection{\texttt{ExpectedImprovementTimesProbOfSuccessScheduler}}
\subsection{\texttt{MinimeseUncertaintyThenExploitScheduler}}

% TODO "Recall that..." bayesian optimisation
% TODO talk about averaging over samples
% TODO figures to explain different acquisition strategies





\chapter{Evaluation}
% TODO point cloud/graph that shows that one is better at first, then the other (for data vs score and time vs score)
\textit{This chapter describes the results that our system achieved. It also lists challenges that we faced during its development and limitations it currently has.}

% TODO mention how common logreg/rndforest are
% TODO PROGRAM skewed datasets with lots of pos, few neg


% TODO evaluate kernel (model evidence etc)
% TODO for 2d: plot variance
 
% TODO optimisation is faster and produces only one mean/sd, easier to use
% using the \texttt{checkgrad.m}
% compare optimisation/sampling adv/disadv
% TODO rasmussen: local maxima correspond to a particular interpretation of the data

\section{Data sources}
The second source of datasets we used was created synthetically using scikit-learn's \texttt{make\_classification} function. This function takes a number of parameters that define the dataset, such as the number of samples, the number of features and the number of classes and creates data according to these parameters. 

The fine grained control over the datasets that this function allows was very valuable to us, as it allowed us to see how our system handles datasets of different nature. We wrote a small script to enable us to quickly create datasets and see how logistic regression and random forest perform on subsets of these datasets.

% TODO actually mention this
A second source of datasets were drawn from the list of datasets that were used in the development of Auto-WEKA as mentioned in the Related Work chapter. The authors of Auto-WEKA made the datasets used to test Auto-WEKA available on their website\footnote{They can be downloaded at http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/}.

These datasets originally came from other sources \cite{Lichman:2013, Larochelle:2007:EED:1273496.1273556, Krizhevsky09learningmultiple} and were converted by the Auto-WEKA creators to WEKA's own dataformat, the Attribute-Relation File Format (\texttt{arff}). This made it very convenient for us to use these datasets as they are originally in a wide range of differing data formats and would have had to have individual import routines written for them. 

Besides converting them to the \texttt{arff} format, the authors of the Auto-WEKA paper also split them up in a training and a test set. As we described in the Implementation chapter, we immplement k-fold-cross-validation and therefore needed all the data in a single set. To achieve this, a small ruby script was written to merge the \texttt{train.arff} and \texttt{test.arff} files for each dataset.

% TODO show table with size (MB), #features, #samples, #classes

While some of the datasets in this collection are entirely numerical, some have textual features while still others have a mixture of strings and numbers as features. An example of this is the \texttt{germancredit} dataset which has a feature \texttt{credit\_history} with possible values such as \texttt{"all paid"} and \texttt{"critical/other existing credit"} and a feature,  \texttt{age}, which is a number.

When using these datasets, we load the \texttt{arff} files using SciPy's \texttt{scipy.io.arff.loadarff} function. As scikit-learn expects the features to all be numerical, we convert the data using a one-of-k encoding.

% TODO kein konjunktiv
This encoding expands every feature of type string with $n$ possible values into $n$ boolean typed features. This means that a feature such as \texttt{other\_payment\_plans}, also taken from the \texttt{germancredit} dataset, with possible values \texttt{bank}, \texttt{stores} and \texttt{none} would be converted to three separate features, \texttt{other\_payment\_plans=bank}, \texttt{other\_payment\_plans=stores} and \texttt{other\_payment\_plans=none}. For every sample in the dataset, one of these features would be set to $1$ and all others to $0$

Another issue we faced when using the Auto-WEKA datasets is that the \texttt{arff} file format allows are sparse encoding for datasets that mostly contain zeros. Other datasets contain samples with missing values. SciPy's \texttt{loadarff} routine is not currently able to open either of these two types of \texttt{arff} files these files.

% TODO explain why this is not a problem: we select datasets already b/c the small ones are too small to further increase data; the interesting ones work?

\section{Results}

\subsection{Hyperparameter selection}
We generally found both of the methods we use for hyperparameter selection, sampling and optimisation, to return satisfying results. As is in the nature of optimisation, however, it is prone to overfit. This was  an issue especially in the early phases of data collection when only little data was available.

One way we addressed this problem was by decreasing the number of data points collected during the burn in phase. As the subsets used in this phase are a small percentage of the whole dataset, this only adds marginally to the time that is spent on burn-in.

% TODO is this really true? verify
Although this addressed the issue of overfitting on too little data, sampling hyperparameters still proved more robust and less likely to return models that clash with the expectations that we had looking at the data.






% TODO show case where one function is better initially but then gets overtaken
\section{Testing}
\section{Limitations}
\section{Challenges}

% TODO matlab, json, for single element lists, an empty [] is added

Implementing the gradients of the hyperparameters to our kernel proved to be challenging. Even though we are not currently using them, we originally assumed we would use GPML's optimisation function which relies on gradient information. The difficulty of implementing them was compounded by the fact that kernels receive parameters in log space and that we reparameterised our kernel which made the terms more complex.

To test our gradient implementation, we used a script provided by the creators of GPML that checks kernel gradients\footnote{This script can be downloaded at http://learning.eng.cam.ac.uk/carl/code/minimize/checkgrad.m}.


% TODO more than one class in training set error

% TODO numerical accuracy for EI when sd is very small and peak is under lower bound, i.e. prob of impr is very small, get infs and nans
%- model evidence
%- testing
%- how well it performs
% TODO "hinge" on my computer
% TODO kernel "cliffs"
% TODO optimisation gets overfits, too much confidence in one interpretation
% TODO gradients. difficult b/c of log space (and long term)
\chapter{Conclusion and future work} 
\section{Conclusions}
In this project, we have succeeded in
\begin{itemize}
	\item creating a covariance function for Gaussian processes that model the exponential behaviour of learning algorithm accuracy as a function of appoximation parameters
	\item developing heuristics that make use of this kernel to create models under run time constraints
\end{itemize}


\section{Future work}
One straightforward way our system could be extended is by considering more algorithms beside logistic regression and random forest. Scikit-learn contains implementations of a great number of other machine learning algorithms that could be added to the current list of algorithms with little additional work. One challenge in this is to make the graphical output of the program stay readable while more models and lines are added.

Similarly, the program could be extended to consider additional hyperparameters besides the percentage of data. This would likely require entirely new ways of visualising the output as it is very difficult to plot functions that take more than one argument.


% TODO additional appr params. the kernel is already implemented

% TODO Depending on the length of your work, and  how well you write, you may not need a summary here. You will generally want to draw some conclusions, and point to potential future work. 
% TODO transfer learning? to avoid burn in



\appendix
\singlespacing

\bibliographystyle{unsrt} 
\bibliography{references}


\end{document}
