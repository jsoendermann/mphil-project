%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%%
%%
%%   SMH, May 2010. 

% TODO compare with other approaches (matern kernel)
% TODO don't stop time spent scoring
% TODO roc_auc_score to score
% TODO convert sparse arff files to non sparse
% TODO rnd forest is bad at data thats heavily pos or neg
% TODO describe gradent problems
% TODO describe classifier scoring mechanism
% TODO optimisation: gpml's didn't work, rolled my own based on slice optimisation; explain grad based optimi, show nlml func to justify that it's simple
% TODO implement better optimiser based on gradients
% TODO comparing model evidence
% TODO look at spearmint
% TODO Nearest Neighbors Classification
% TODO describe ranges

% TODO UML diagram for schedulers
% TODO If you use purely-numeric bibliographic references, do not forget to still mention authorsâ€™ surnames
% TODO describe kernel problems
% TODO establish the words scheduler and approx params early on

% TODO datasets: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
% TODO explain datasets w/ images if they are interesting
% TODO Appendices should be avoided where possible, but may be appropriate in some cases
% TODO mention ec2

% TODO Where a project has as its main aim the production of a piece of software, the project report should state clearly what test procedures were adopted and should include test output
% TODO The report should explicitly describe the starting point for the project, making clear what existing software or other resources were used

% TODO Datasets
%- Autoweka data taken from their website (how were those datasets selected?)
%- merge training and test arffs using weka cmd line
%python:
%- load using scipy.io.arff.loadarff
%- vectorise (one-of-K) using sklearn.feature_extraction.DictVectorizer

\documentclass[a4paper,12pt,twoside,openright]{report}

% Diagrams
\usepackage{graphicx}

\def\authorname{Jan S\"ondermann\xspace}
\def\authorcollege{Selwyn College\xspace}
\def\authoremail{jjes2@cam.ac.uk}
% TODO Title
\def\dissertationtitle{@@@Title@@@}
% TODO word count
\def\wordcount{0}

\usepackage{chngpage}
\usepackage{calc}
\usepackage{epsfig,graphicx,parskip,setspace,tabularx,xspace} 

%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

%% START OF MAIN TEXT 

\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 

- context
- motivation
- general overview
% TODO context (why did you do it (motivation), what was the hoped-for outcome (aims) --- as well as trying to give a brief overview of what you actually did)


% TODO It's often useful to bring forward some ``highlights'' into  this chapter (e.g.\ some particularly compelling results, or  a particularly interesting finding). 

% TODO It's also traditional to give an outline of the rest of the document, although without care this can appear formulaic and tedious. Your call. 


\chapter{Background}
\section{Machine Learning}
\section{Algorithms}
\subsection{Gaussian Processes}
\subsection{Random Forests and Logistic Regression}

as this is meta machine learning, i have to introduce @normal machine learning at a high level@
algos + their imple (rf, lr, gps)

- ML + algos
- GPs/GPML (marginal likelihood)
- mcmc/sampling

% describe scheduling

% TODO A more extensive coverage of what's required to understand your work. In general you should assume the reader has a good undergraduate degree in computer science, but is not necessarily an expert in the particular area you've been working on. Hence this chapter may need to summarise some ``text book'' material. 



\chapter{Related Work} 

- hyper param optim
- multi armed bandits

% TODO This chapter covers relevant (and typically, recent) research which you build upon (or improve upon). There are two complementary goals for this chapter: - to show that you know and understand the state of the art; and to put your work in context

% TODO Ideally you can tackle both together by providing a critique of related work, and describing what is insufficient (and how you do better!)









\chapter{Design and Implementation} 



\textit{This chapter describes the theoretical and practical aspects system we developed. It firsts gives a high level overview of the system architecture before describing in detail the two main parts of the program: modelling performance and scheduling.}

\section{Architecture} % TODO maybe something like "overview"
\subsection{Key concepts}
\subsubsection{Approximation parameters}
\subsubsection{Scheduling}
\subsection{High level architecture}
Our system is designed in a two tiered fashion. Tier 1 creates models of the data using learning algorithms that are parameterised with approximation parameters. The training time and prediction accuracy of these models makes up the input to tier 2. This second layer first creates a meta-model of the model performances of the models in tier 1. This meta-model is then used by the scheduling part of the program to select the algorithm and approximation parameters for the next iteration.

Figure \ref{architecture} shows the system architecture diagrammatically. 

@alternate back and forth

@tier 1 is standard ml stuff (using sklearn), our contribution is tier 2, has two parts: modelling and scheduling, now describe in detail

\begin{figure}[!ht]
  \begin{adjustwidth}{-\oddsidemargin-2in}{-\rightmargin-1.5in}
    \centering
    \includegraphics[trim=0 0 0 0,clip,width=0.9\paperwidth]{figures/architecture.pdf}
    
  \end{adjustwidth}
  \caption{High level architecture, shown after three iterations. Data is coloured red, machine learning algorithms are coloured blue}
    \label{architecture}
\end{figure}




\section{Modelling Model Performance}
To make decisions about where to 


\subsection{Data Collection}


\subsection{Kernel}
\subsection{Selecting Kernel Hyperparameters}
% TODO maybe call this model selection
To fit a Gaussian process to a given set of data points, one needs to find appropriate values for the hyperparameters to its covariance function. There two main methods to achieve this are sampling and optimisation. We use both of these methods as described @@. An evaluation of their advantages and disadvantages can be found in the Evaluation chapter.

\subsubsection{Sampling}
The Bayesian way to handle hyperparameters is to integrate them out. 
% TODO add maths on integrating out hyperparams

To approximate this integral, MCMC methods are
% TODO maths on MCMC

To sample from @ we use slice sampling. adv/disadv: \cite{neal2003}
% TODO why slice sampling and not another method



\subsubsection{Optimisation}
Another approach to find appropriate hyperparameters is to optimise the marginal likelihood of the model with respect to its hyperparameters.

GPML comes with an optimisation function that is meant to be used to optimise hyperparameters. It relies on gradient information which we implemented for the exponential mixture kernel. We were, however, not able to use this function with our kernel as it would terminate after a very small number of steps having barely moved from the initial values @not being close to minimum@. We suspect the reason for this to be numerical instability problems that make the function @wiggly@ and creates @.

Figure @ shows an example of a marginal likelihood function that we tried to optimise with GPML's \texttt{minimize}. @looks easy to optimise and 
% TODO for this subsection: show plots of likelihood function



% TODO maybe include pseudocode for every step?

% TODO optimisation is faster and produces only one mean/sd, easier to use

% TODO mention numerical instability
% TODO this algorithm is "too clever"
% TODO show figs where optimisation fails

we then tried slice sampling, a sampling method that does not optimise the parameter but instead integrates them out and @draws samples from the posteriour?@. this worked quite well in the limited number of cases we tested.

because we wanted an optimisation technique @why? fast. ease of use@, we decided to implement an optimisation method that works similarly to slice sampling @is there any literature on this?@ this algorithm works by considering a window around the current value, updating if a smaller y was found and narrowing the window size otherwise. pseudo code for this is shown in X.

we also implemented a gradient based approach (b/c otherwise moving in a direction that's not axis aligned is slow) but that never really worked (describe nonetheless, also describe attempts to make it work

this worked fairly well but not in @more complex? noisier@ examples. describe problem with cholesky decomposition. noise param was optimised to very low value, overfitting? show diagram

After observing that restarting this algorithm with data for which it failed with a different random seed sometimes successfully optimised the parameters, we implemented a wrapper around our algorithm that reruns it multiple times, ignoring the result for cholesky errors.

this produced satisfying results but took a long time to optimise. to cut down the run time, we added a final improvement by making the algorithm terminate early if y doesn't get updated in 5 successive iterations. this happens often and makes the algorithm fast. figure X shows pseudocode for this final version of the algorithm




%\section{Approximation parameters and performance}



section overview
1. modelling
to model, two things are necessary: 1. appropriate kernel function, 2. inference
for inference, we use optimisation
- optimisation
- 1d/2d

to achieve what we want to achieve (@which is what? read proposal and michael jordan paper again@) the first ingredient is to be able to model param->score and param->time.

- describe data collection
-- iface

%show diagrams, talk about the relationship. is it always linear for time (no, svm, O(n^2-3)), is it always exponential for score

time easy: linear (mention kernel)

% TODO different names for these two subsections
%\subsection{Implementing an appropriate kernel}
% TODO describe problems with kernel (maybe in evaluation)
%\subsection{GP hyperparameters}



\section{Scheduling} 



\chapter{Evaluation} 
% using the \texttt{checkgrad.m}
% compare optimisation/sampling adv/disadv
% TODO rasmussen: local maxima correspond to a particular interpretation of the data

\section{Data sources}
- explain where the data came from and how it was preprocessed
\section{Testing}
\section{Limitations}
\section{Challenges} % TODO most of this was prob mentioned in design&impl

- model evidence
- testing
- how well it performs


\chapter{Conclusion and future work} 
\section{Conclusions}
\section{Future work}
% TODO Depending on the length of your work, and  how well you write, you may not need a summary here. You will generally want to draw some conclusions, and point to potential future work. 




\appendix
\singlespacing

\bibliographystyle{unsrt} 
\bibliography{references}


\end{document}
